{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485acc5a",
   "metadata": {},
   "source": [
    "# 4.1 데이터에서 학습한다!\n",
    " 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. -> 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a6785",
   "metadata": {},
   "source": [
    "## 4.1.1 데이터 주도 학습\n",
    " 기계학습은 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만듦 -> 기계학습의 중심에는 데이터가 존재\n",
    " 기계학습은 수집한 데이터로부터 패턴을 찾으려 시도, 게다가 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님<br><br>\n",
    " 이미지에서 **특징(feature)**을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 여기서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할수 있도록 설계된 변환기를 가리킨다. 이미지의 특징은 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIPT, SURF, HOG 등의 특징을 많이 사용한다. 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다.\n",
    " <br> <br>\n",
    " 이와 같은 기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당한다. 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계한다. 이 말은 문제에 적합한 특징을 쓰지 않으면(혹은 특징을 설계하지 않으면) 좀처럼 좋은 결과를 얻을 수 없다는 뜻이다. 예를 들어 개의 얼굴을 구분하려 할 때는 숫자를 인식할 때와는 다른 특징을 '사람'이 생각해야 할지도 모른다. 즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 '사람'이 적절한 특징을 생각해내야 한다.<br><br>\n",
    " \n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FETLRX%2Fbtq1YkKAuD5%2F5H0wJ3G7TotarX3S1hmJJK%2Fimg.png\"  width=\"400px\" height=\"400px\"></img><br>\n",
    "회색 블록은 사람이 개입하지 않음을 뜻한다.<br>\n",
    "신경망은 이미지를 '있는 그대로' 학습한다. 두 번째 접근 방식(특징과 기계학습 방식)에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습할 것이다.<br><br>\n",
    "신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다. 예를 들어 '5'를 인식하는 문제든, '개'를 인식하는 문제든, 아니면 '사람의 얼굴'을 인식하는 문제든, 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도한다. 즉, 신경망의 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다.\n",
    "<br><br>\n",
    "**NOTE_** 딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 한다. 종단간은 '처음부터 끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54928d0",
   "metadata": {},
   "source": [
    "## 4.1.2 훈련 데이터와 시험 데이터\n",
    " 기계학습에서 문제는 데이터를 **훈련 데이터**와 **시험 데이터**로 나눠 학습과 실험을 수행하는 것이 일반적이다. 우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 그 다음 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가한다. -> 범용 능력을 평가하기 위해서\n",
    "<br><br>\n",
    " 범용 능력은 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력이다. \n",
    " <br><br>\n",
    " **오버피팅(overfitting)**은 훈련 데이터에만 높은 정확률을 보이고, 시험 데이터에서는 오차가 있는 경우를 뜻한다. 이는 일반화가 잘 되지 않았다는 뜻이다. 모델이 복잡하거나 데이터의 수가 적을 때(특정한 특징을 가진 데이터만으로 학습할 때) 발생한다. <br>\n",
    "  이를 해결하기 위해 모델을 단조롭게 하거나(dropout, 가중치 규제(Regularization) 등) 데이터 양을 늘린다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f7aaa",
   "metadata": {},
   "source": [
    "# 4.2 손실 함수\n",
    " 신경망은 학습이 잘 되었는지 안되었는지 확인하기 위한 지표를 기준으로 최적의 매개변수 값을 탐색한다. 이 지표를 **손실함수(loss funtion)** 라고 한다.<br>\n",
    " 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.<br>\n",
    " 손실함수를 줄이는게 우리의 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0880df",
   "metadata": {},
   "source": [
    "## 4.2.1 평균 제곱 오차\n",
    " 가장 많이 쓰이는 손실 함수는 **평균 제곱 오차(mean squared error)**이다.<br>\n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdCDpqv%2Fbtq102Ja1sN%2FadDK0g6c4q8nutIvh9bnyk%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    " 여기서 y_k는 신경망의 출력(신경망이 추정한 값, 예측값), t_k는 정답 레이블, k는 데이터의 차원 수를 나타낸다. 3.6절에서 했던 손글씨 숫자 인식 예에서 y_k와 t_k는 다음과 같은 원소 10개짜리 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a2d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t_k = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b47df5",
   "metadata": {},
   "source": [
    "이 배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 '0', '1', '2', ... 일 때의 값이다. 여기서 y_k는 소포트맥스 함수의 출력이다. <br>\n",
    "이 예에서 이미지가 '0'일 확률은 0.1, '1'일 확률은 0.05, '2'일 확률은 0.6이라고 해석된다. 한편 정답 레이블인 t_k는 정답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 표기한다. 여기서는 숫자 '2'에 해당하는 원소의 값이 1이므로 정답이 '2'임을 알 수 있다. 이처럼 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법을 **원-핫 인코딩**이라 한다.\n",
    "<br><br>\n",
    "평균 제곱 오차는 위 식과 같이 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차(y_k - t_k)를 제곱한 후, 그 총합을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f7443a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y_k, t_k):\n",
    "    return 0.5 * np.sum((y_k - t_k)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fdee646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예 1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c2069c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예 2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y_k = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6b6b6",
   "metadata": {},
   "source": [
    "첫 번째 예는 정답이 '2'고 신경망의 출력도 '2'에서 가장 높은 경우이다.<br>\n",
    "두 번째 예는 정답이 똑같이 '2'지만, 신경망의 출력은 '7'에서 가장 높다.<br> \n",
    "<br>\n",
    "이 실험의 결과로 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다. 즉, 평균 제곱 오차를 기준으로 첫 번째 추정 결과가 (오차가 더 작으니) 정답에 더 가까울 것으로 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc28215",
   "metadata": {},
   "source": [
    "## 4.2.2 교차 엔트로피\n",
    "또 다른 손실 함수로서 **교차 엔트로피 오차(cross entropy error, CEE)**도 자주 사용한다.<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNAFf7%2Fbtq1WKpHPxE%2FEuafGjZOGD0ddh7yHoWybK%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    " log는 밑이 e인 자연로그이다. 실질적으로 정답일 때의 추정(t_k가 1일 때의 y_k)의 자연로그를 계산하는 식이 된다.<br>\n",
    " 예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는 -log0.6 = 0.51이 된다. 또한 같은 조건에서 신경망 출력이 0.1이라면 -log0.1 = 2.30이 된다. 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.<br><br>\n",
    " 자연로그 y = logx 의 그래프<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FchMSBp%2Fbtq11yODoRj%2Fsn4Z8DyXUcZpbkELx4lQkk%2Fimg.png\"  width=\"600px\" height=\"600px\"></img><br>\n",
    "이 그림에서 보듯이 x가 1일 때는 y는 0이 되고 x가 0에 가까워질수록 y의 값은 점점 작아진다. 위 식도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56453cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_k, t_k):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t_k * np.log(y_k + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd9a16",
   "metadata": {},
   "source": [
    "위 코드에서 아주 작은 값인 delta를 더했다. 이는 np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문이다. 아주 작은 값을 더해서 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa2a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t_k = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3986a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6762c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5948ec57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1221b",
   "metadata": {},
   "source": [
    "첫 번째 예는 정답일 때의 출력이 0.6인 경우로 오차는 약 0.51이다. 그 다음은 정답일 때의 출력이 더 낮은 0.1인 경우로 오차는 무려 2.3이다.<br>\n",
    "즉, 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 평균 제곱 오차의 판단과 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8463",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2dd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
