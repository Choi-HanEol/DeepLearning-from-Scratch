{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485acc5a",
   "metadata": {},
   "source": [
    "# 4.1 데이터에서 학습한다!\n",
    " 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. -> 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a6785",
   "metadata": {},
   "source": [
    "## 4.1.1 데이터 주도 학습\n",
    " &nbsp;기계학습은 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만듦 -> 기계학습의 중심에는 데이터가 존재\n",
    " 기계학습은 수집한 데이터로부터 패턴을 찾으려 시도, 게다가 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님<br><br>\n",
    " &nbsp;이미지에서 **특징(feature)**을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 여기서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할수 있도록 설계된 변환기를 가리킨다. 이미지의 특징은 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIPT, SURF, HOG 등의 특징을 많이 사용한다. 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다.\n",
    " <br> <br>\n",
    " &nbsp;이와 같은 기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당한다. 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계한다. 이 말은 문제에 적합한 특징을 쓰지 않으면(혹은 특징을 설계하지 않으면) 좀처럼 좋은 결과를 얻을 수 없다는 뜻이다. 예를 들어 개의 얼굴을 구분하려 할 때는 숫자를 인식할 때와는 다른 특징을 '사람'이 생각해야 할지도 모른다. 즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 '사람'이 적절한 특징을 생각해내야 한다.<br><br>\n",
    " \n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FETLRX%2Fbtq1YkKAuD5%2F5H0wJ3G7TotarX3S1hmJJK%2Fimg.png\"  width=\"400px\" height=\"400px\"></img><br>\n",
    "&nbsp;회색 블록은 사람이 개입하지 않음을 뜻한다.<br>\n",
    "신경망은 이미지를 '있는 그대로' 학습한다. 두 번째 접근 방식(특징과 기계학습 방식)에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습할 것이다.<br><br>\n",
    "&nbsp;신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다. 예를 들어 '5'를 인식하는 문제든, '개'를 인식하는 문제든, 아니면 '사람의 얼굴'을 인식하는 문제든, 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도한다. 즉, 신경망의 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다.\n",
    "<br><br>\n",
    "**NOTE_** 딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 한다. 종단간은 '처음부터 끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54928d0",
   "metadata": {},
   "source": [
    "## 4.1.2 훈련 데이터와 시험 데이터\n",
    " &nbsp;기계학습에서 문제는 데이터를 **훈련 데이터**와 **시험 데이터**로 나눠 학습과 실험을 수행하는 것이 일반적이다. 우선 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다. 그 다음 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가한다. -> 범용 능력을 평가하기 위해서\n",
    "<br><br>\n",
    " &nbsp;범용 능력은 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력이다. \n",
    " <br><br>\n",
    " &nbsp;**오버피팅(overfitting)**은 훈련 데이터에만 높은 정확률을 보이고, 시험 데이터에서는 오차가 있는 경우를 뜻한다. 이는 일반화가 잘 되지 않았다는 뜻이다. 모델이 복잡하거나 데이터의 수가 적을 때(특정한 특징을 가진 데이터만으로 학습할 때) 발생한다. <br>\n",
    "  이를 해결하기 위해 모델을 단조롭게 하거나(dropout, 가중치 규제(Regularization) 등) 데이터 양을 늘린다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f7aaa",
   "metadata": {},
   "source": [
    "# 4.2 손실 함수\n",
    " &nbsp;신경망은 학습이 잘 되었는지 안되었는지 확인하기 위한 지표를 기준으로 최적의 매개변수 값을 탐색한다. 이 지표를 **손실함수(loss funtion)** 라고 한다.<br>\n",
    " &nbsp;손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.<br>\n",
    " &nbsp;손실함수를 줄이는게 우리의 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0880df",
   "metadata": {},
   "source": [
    "## 4.2.1 평균 제곱 오차\n",
    " 가장 많이 쓰이는 손실 함수는 **평균 제곱 오차(mean squared error)**이다.<br>\n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdCDpqv%2Fbtq102Ja1sN%2FadDK0g6c4q8nutIvh9bnyk%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    " 여기서 y_k는 신경망의 출력(신경망이 추정한 값, 예측값), t_k는 정답 레이블, k는 데이터의 차원 수를 나타낸다. 3.6절에서 했던 손글씨 숫자 인식 예에서 y_k와 t_k는 다음과 같은 원소 10개짜리 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7a2d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t_k = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b47df5",
   "metadata": {},
   "source": [
    "&nbsp;이 배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 '0', '1', '2', ... 일 때의 값이다. 여기서 y_k는 소포트맥스 함수의 출력이다. <br>\n",
    "이 예에서 이미지가 '0'일 확률은 0.1, '1'일 확률은 0.05, '2'일 확률은 0.6이라고 해석된다. 한편 정답 레이블인 t_k는 정답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 표기한다. 여기서는 숫자 '2'에 해당하는 원소의 값이 1이므로 정답이 '2'임을 알 수 있다. 이처럼 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법을 **원-핫 인코딩**이라 한다.\n",
    "<br><br>\n",
    "평균 제곱 오차는 위 식과 같이 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차(y_k - t_k)를 제곱한 후, 그 총합을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f7443a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y_k, t_k):\n",
    "    return 0.5 * np.sum((y_k - t_k)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fdee646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예 1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0c2069c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예 2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y_k = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6b6b6",
   "metadata": {},
   "source": [
    "&nbsp;첫 번째 예는 정답이 '2'고 신경망의 출력도 '2'에서 가장 높은 경우이다.<br>\n",
    "&nbsp;두 번째 예는 정답이 똑같이 '2'지만, 신경망의 출력은 '7'에서 가장 높다.<br> \n",
    "<br>\n",
    "&nbsp;이 실험의 결과로 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다. 즉, 평균 제곱 오차를 기준으로 첫 번째 추정 결과가 (오차가 더 작으니) 정답에 더 가까울 것으로 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc28215",
   "metadata": {},
   "source": [
    "## 4.2.2 교차 엔트로피\n",
    "&nbsp;또 다른 손실 함수로서 **교차 엔트로피 오차(cross entropy error, CEE)**도 자주 사용한다.<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNAFf7%2Fbtq1WKpHPxE%2FEuafGjZOGD0ddh7yHoWybK%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    " &nbsp;log는 밑이 e인 자연로그이다. 실질적으로 정답일 때의 추정(t_k가 1일 때의 y_k)의 자연로그를 계산하는 식이 된다.<br>\n",
    " 예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는 -log0.6 = 0.51이 된다. 또한 같은 조건에서 신경망 출력이 0.1이라면 -log0.1 = 2.30이 된다. 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.<br><br>\n",
    " 자연로그 y = logx 의 그래프<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FchMSBp%2Fbtq11yODoRj%2Fsn4Z8DyXUcZpbkELx4lQkk%2Fimg.png\"  width=\"600px\" height=\"600px\"></img><br>\n",
    "&nbsp;이 그림에서 보듯이 x가 1일 때는 y는 0이 되고 x가 0에 가까워질수록 y의 값은 점점 작아진다. 위 식도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56453cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_k, t_k):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t_k * np.log(y_k + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd9a16",
   "metadata": {},
   "source": [
    "&nbsp;위 코드에서 아주 작은 값인 delta를 더했다. 이는 np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문이다. 아주 작은 값을 더해서 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2aa2a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t_k = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3986a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6762c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_k = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5948ec57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y_k), np.array(t_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1221b",
   "metadata": {},
   "source": [
    "첫 번째 예는 정답일 때의 출력이 0.6인 경우로 오차는 약 0.51이다. 그 다음은 정답일 때의 출력이 더 낮은 0.1인 경우로 오차는 무려 2.3이다.<br>\n",
    "즉, 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 평균 제곱 오차의 판단과 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8463",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습\n",
    " &nbsp;기계학습 문제는 훈련 데이터를 사용해 학습한다. 더 구체적으로 말하면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다. 이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것이다. <br><br>\n",
    " &nbsp;지금까지 데이터 하나에 대한 손실 함수만 생각했다. 이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자. <br>아래는 교차 엔트로피 오차에 대한 식이다.<br>\n",
    "  <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FckGuuv%2Fbtq1XlQuy6b%2F7VMOsKXz7Igni2KtvHVFf1%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    "&nbsp;데이터가 N개라면 t_nk는 n번째 데이터의 k번째 값을 의미한다(y_nk는 신경망의 출력, t_nk는 정답 레이블). 마지막에 N으로 나누어 정규화하고 있다. N으로 나눔으로써 '평균 손실 함수'를 구하는것이다. 이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다. 예를 들어 훈련 데이터가 1,000개든 10,000개든 상관없이 평균 손실 함수를 구할 수 있다.<br><br>\n",
    "&nbsp;그런데 빅데이터 수준의 수백만에서 수천만도 넘는 데이터에 대한 손실 함수의 합을 구하려면 시간이 오래 걸린다. 이 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않다. 이런 경우 데이터 일부를 추려 전체의 '근사치'로 이용할 수 있다. 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행한다. 이 일부를 **미니배치(mini-batch)**라고 한다. 가령 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 것이다. 이러한 학습 방법을 **미니배치 학습**이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cad2dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff0312",
   "metadata": {},
   "source": [
    "&nbsp;MNIST 데이터셋을 읽어온다. 읽어올 때 one_hot_label=True로 지정하여 원-핫 인코딩으로, 즉 정답 위치의 원소만 1이고 나머지는 0인 배열을 얻는다. 훈련 데이터는 60,000개이고, 입력 데이터는 784열(원래는 28 x 28)인 이미지 데이터임을 알 수 있다. 또, 정답 레이블은 10차원 데이터이다.<br><br>\n",
    "&nbsp;훈련 데이터에서 무작위로 10장만 빼보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bc7aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db1e94ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29409, 55702, 33563, 10821,  6479,  9980,    29, 29594, 55309,\n",
       "       51014])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e30e4",
   "metadata": {},
   "source": [
    "np.random.choice()는 지정한 범위의 수 중에서 무작위로 원하는 개수만 꺼낼 수 있다. 가령 np.random.choice(60000, 10)은 0 이상 60000 미만의 수 중에서 무작위로 10개를 골라낸다. 이 함수가 출력한 배열을 미니배치로 뽑아낼 데이터의 인덱스로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215f75f",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3fb890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_k, t_k):\n",
    "    if y_k.ndim == 1:\n",
    "        t = t.reshape(1, t_k.size)\n",
    "        y = y.reshape(1, y_k.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t_k * np.log(y_k)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ab53e",
   "metadata": {},
   "source": [
    "&nbsp;y_k가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는경우는 reshape 함수로 데이터의 형상을 바꿔준다. 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다.<br><br>\n",
    "&nbsp;정답 레이블이 원-핫 인코딩이 아니라 '2'나 '7' 등의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다움과 같이 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17372443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_k, t_k):\n",
    "    if y_k.ndim == 1:\n",
    "        t = t.reshape(1, t_k.size)\n",
    "        y = y.reshape(1, y_k.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y_k[np.arange(batch_size), t_k])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c1ee",
   "metadata": {},
   "source": [
    "&nbsp;이 구현에서는 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다. 다시 말하면 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다. <br><br>\n",
    "&nbsp;np.log(y_k[np.arange(batch_size), t_k])를 간단히 설명하자면 np.arange(batch_size)는 0부터 batch_size - 1 까지 배열을 생성한다. 즉, batch_size가 5이면 np.arange(batch_size)는 [0, 1, 2, 3, 4]라는 넘파이 배열을 생성한다. t_k에는 레이블이 [2, 7, 0, 9, 4]와 같이 저장되어 있으므로 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다. -> [y_k[0,2], y_k[1,7], y_k[2,0], y_k[3,9], y_k[4,4]] 인 넘파이 배열 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edb3e4",
   "metadata": {},
   "source": [
    "# 4.3 수치 미분 \n",
    "&nbsp;경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151a1cb",
   "metadata": {},
   "source": [
    "## 4.3.1 미분\n",
    "&nbsp;미분은 한순간의 변화량을 표시한 것.<br>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkGgNtEESDEJoWmrItNfxt9WJjbKUzYPC4vhkG1h0T6wfXuRYKjkpidxFlWC8EVotiPA&usqp=CAU\"  width=\"300px\" height=\"300px\"></img><br>\n",
    "&nbsp;좌변은 f(x)의 x에 대한 미분(x에 대한 f(x)의 변화량). 즉, x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미.<br>\n",
    "우변의 lim h->0은 시간을 뜻하는 h를 한없이 0에 가깝게 한다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aac604f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 식을 곧이곧대로 구현\n",
    "# 나쁜 구현 예\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7531fd5",
   "metadata": {},
   "source": [
    "### 첫번째 문제\n",
    "&nbsp;10e-50은 0.00...1 형태에서 0이 50개라는 의미이다. 위 방식은 **반올림 오차**를 일으킨다.<br>\n",
    "반올림 오차는 작은 값(가령 소수점 8자리 이하)이 생략되어 최종 계산 결과에 오차가 생기게 한다. 파이썬에서의 반올림 오차로는 다음과 같은 예가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e0442d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425c810",
   "metadata": {},
   "source": [
    "&nbsp;이와 같이 1e-50을 float32형(32비트 부동소수점)으로 나타내면 0.0이 되어, 올바로 표현할 수 없다. 너무 작은 값을 이용하면 컴퓨터로 계산하는 데 문제가 된다. 이 미세한 값 h로 10^(-4)을 이용하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cf29d",
   "metadata": {},
   "source": [
    "### 두번째 문제\n",
    "&nbsp;함수 f의 차분(임의 두 점에서의 함수 값들의 차이)과 관련한 것이다. x + h와 x사이의 함수 f의 차분을 계산하고 있는데 이는 오차가 있다.<br>\n",
    "**진정한 미분**은 (x + h)와 x사이의 기울기에 해당. 그래서 진정한 미분(진정한 접선)과 위 구현의 값은 일치하지 않음. 이 차이는 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계이다.<br><br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FebWi25%2Fbtq10KicVzZ%2FuD4G2kLbpJEElk7gQ59zL0%2Fimg.jpg\"  width=\"300px\" height=\"300px\"></img><br>\n",
    "&nbsp;이 오차를 줄이기 위해 (x + h)와 (x - h)일 때의 함수 f의 차분을 계산하는 방법을 사용한다. 이 차분은 x를 중심으로 그 전호의 차분을 계산한다는 의미에서 **중심 차분** 혹은 **중앙 차분**이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bd41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64b58b",
   "metadata": {},
   "source": [
    "## 4.3.2 수치 미분의 예\n",
    "&nbsp;아주 작은 차분으로 미분하는 것을 **수치 미분**이라고 한다.(근사치로 계산)<br><br>\n",
    "y = 0.01x^2 + 0.1x<br><br>\n",
    "을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c0d36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e11acf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)    # 0에서 20까지 0.1 간격의 배열 x를 만든다\n",
    "y = function_1(x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c19f6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x = 5일 때와 10일 때 이 함수의 미분을 계산하자\n",
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bac65",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분\n",
    "f(x0, x1) = x0^2 + x1^2<br><br>\n",
    "&nbsp;위 식은 앞의 예와 달리 변수가 2개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "473a5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "# 또는return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03834b",
   "metadata": {},
   "source": [
    "&nbsp;위처럼 변수가 여럿인 함수에 대한 미분을 **편미분**이라고 한다.<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcSNsH1%2Fbtq1ZFH83qa%2Fj4bsNVGWu1kijCXoKEskK1%2Fimg.jpg\"  width=\"300px\" height=\"300px\"></img><br>\n",
    "f(x0, x1) = x0^2 + x1^2<br><br>\n",
    "에 대한 문제를 풀어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c6680c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4일 때, x0에 대한 편미분을 구하라\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "389282af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4일 때, x1에 대한 편미분을 구하라\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a3536",
   "metadata": {},
   "source": [
    "# 4.4 기울기\n",
    "&nbsp;앞 절의 예에서는 x0과 x1의 편미분을 변수별로 따로 계산했다. 그럼 x0와 x1의 편미분을 동시에 계산하고 싶다면 어떻게 해야할까?<br>\n",
    "&nbsp;모든 변수의 편미분을 벡터로 정리한 것을 **기울기(gradient)**라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54249b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 원소가 모두 0인 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x + h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1e99e",
   "metadata": {},
   "source": [
    "&nbsp;이제 (3,4), (0,2), (3,0)에서의 기울기를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3708d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e2ba2",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuLN5K%2Fbtq1ZECyRMI%2FiOWb1SwbRpgu3DigLepLyK%2Fimg.png\"  width=\"500px\" height=\"500px\"></img><br>\n",
    "&nbsp;위 그림은 위 기울기의 결과에 마이너스를 붙인 벡터를 그린 것이다.<br>\n",
    "기울기는 함수의 '가장 낮은 장소(최솟값)'를 가리키는 것 같다. 최솟값에서 멀어질수록 화살표의 크기가 커짐을 알 수 있다.<br>\n",
    "<br>\n",
    "&nbsp;기울기는 각 지점에서 낮아지는 방향을 가리킨다. 더 정확히 말하자면 **기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 줄이는 방향**이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841d030",
   "metadata": {},
   "source": [
    "## 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb456170",
   "metadata": {},
   "source": [
    "&nbsp;일반적인 문제의 손실 함수는 매우 복잡하다. 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 알아내기가 쉽지않다. 이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다.<br><br>\n",
    "&nbsp;각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기다. 하지만 기울기가 가리키는 곳이 정말로 최소값인지 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다.<br><br>\n",
    "&nbsp;경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 **경사법**이다. 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee2456",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbP6FUT%2Fbtq127DDzmK%2FdqSvlXgScHa308CLIKHko0%2Fimg.jpg\"  width=\"150px\" height=\"150px\"></img><br>\n",
    "&nbsp;경사법의 수식이다. n기호(에타)는 갱신하는 양 (학습률) : 한번의 학습으로 얼마나 갱신할지, 학습률 값은 미리 특정 값으로 정해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "183fd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abd81f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제: 경사법으로 f(x0, x1) = x0^2 + x1^2 의 최솟값을 구하라\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd280f8",
   "metadata": {},
   "source": [
    "&nbsp;초깃값 (-3.0, 4.0)에서 최솟값 탐색을 시작하여, 최종 결과는 (-6.11110793e-10,  8.14814391e-10)으로, 거의 (0, 0)에 가까운 결과이다.<br>이 과정을 그림으로 나타내면 다음과 같다.<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlCGSD%2Fbtq11LVd9lU%2FmdkieKHef3KxriTCJCkU90%2Fimg.png\"  width=\"500px\" height=\"500px\"></img><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed70a2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 큰 예 : lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b936a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 작은 예 : lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8755b",
   "metadata": {},
   "source": [
    "&nbsp;학습률이 너무 크면 큰 값으로 발산<br>\n",
    "&nbsp;학습률이 너무 작으면 거의 갱신되지 않은채 끝남<br><br>\n",
    "**NOTE_** 학습률 같은 매개변스롤 **하이퍼파라미터**라고 한다. 이는 가중치, 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해 '자동'으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야하는 매개변수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f592dcf",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망에서의 기울기\n",
    "&nbsp;신경망 학습에서도 기울기를 구해야한다. 가중치 매개변수에 대한 손실 함수의 기울기<br>\n",
    "아래 예에서는 형상이 2 x 3, 가중치가 W, 손실 함수가 L인 신경망.\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCCjLZ%2Fbtq11zm5BHJ%2FKtmD834pRokDdmn7GX6E1K%2Fimg.jpg\"  width=\"500px\" height=\"500px\"></img><br>\n",
    "W와 편미분 dW의 형상은 같다.<br><br>\n",
    "&nbsp;간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afd2fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t): # x: 입력 데이터, t: 정답 레이블\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "889801cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32302205 -0.6524605  -0.75934854]\n",
      " [-0.58165083 -0.91416848 -1.34963921]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2facfae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32967252 -1.21422793 -1.67028441]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d259c711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) #최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "218f311d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8561755485905402"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9da2c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a783b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35829788  0.14794037 -0.50623826]\n",
      " [ 0.53744683  0.22191056 -0.75935738]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d155e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35829788  0.14794037 -0.50623826]\n",
      " [ 0.53744683  0.22191056 -0.75935738]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c674f",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기\n",
    "&nbsp;신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. 신경망 학습은 다음과 같이 4단계로 수행된다.<br><br>\n",
    "**1단계 - 미니배치**<br>\n",
    "&nbsp;훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.<br><br>\n",
    "**2단계 - 기울기 산출**<br>\n",
    "&nbsp;미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.<br><br>\n",
    "**3단계 - 매개변수 갱신**<br>\n",
    "&nbsp;가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.<br><br>\n",
    "**4단계 - 반복**<br>\n",
    "&nbsp;1~3단계를 반복한다.<br><br>\n",
    "&nbsp;이것이 신경망 학습이 이뤄지는 순서이다. 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법(Stochastic gradient descent, SGD)**이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a140fbb",
   "metadata": {},
   "source": [
    "## 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f52c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): #클래스 초기화\n",
    "        # 가중치 초기화, 정규분포를 따르는 난수, 편향은 0으로 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t): #각 매게변수의 기울기를 계산\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b4d88",
   "metadata": {},
   "source": [
    "&nbsp;TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다. params 변수에는 가중치 매개변수가 저장되는데, 예를 들어 1번째 층의 가중치 매개변수는 params['W1'] 키에 넘파이 배열로 저장된다. 마찬가지로 1번째 층의 편향은 params['b1'] 키로 접근한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "abce935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "916671bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측처리\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a9f1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dfdfd4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.03672074e-05 -7.76753839e-05  1.68384833e-04 ...  2.08513029e-05\n",
      "   6.64765776e-05  1.15948720e-04]\n",
      " [ 1.04364029e-05 -1.46910351e-05  1.22401398e-04 ...  1.74487491e-05\n",
      "   2.32213182e-05  8.80194206e-05]\n",
      " [ 1.04669988e-04 -5.00567321e-05 -6.64927668e-06 ... -1.94720462e-06\n",
      "  -1.13893082e-04 -1.38310097e-04]\n",
      " ...\n",
      " [-1.40588312e-04  5.42029222e-05  8.84374352e-06 ...  6.25488106e-05\n",
      "   7.87667975e-05  3.02857961e-06]\n",
      " [ 3.98933131e-05 -5.98979311e-06  7.83823406e-05 ...  2.06503459e-05\n",
      "   2.94304137e-05  2.11344942e-06]\n",
      " [-1.63717122e-04 -1.64696901e-05  1.31246918e-04 ...  1.06494098e-04\n",
      "   1.24844108e-04  1.72679790e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(grads['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b3e78",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현하기\n",
    "&nbsp;미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5252c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3be77",
   "metadata": {},
   "source": [
    "&nbsp;미니배치 크기를 100으로 했다. 즉, 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다. 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. 경사법에 의한 갱신 횟수(반복 횟수)를 10,000번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고, 그 값을 배열에 추가한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48625a74",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가하기\n",
    "&nbsp;신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야 한다. 즉,**오버피팅**을 일으키지 않는지 확인해야 한다. **오버피팅(과적합)**은 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻이다.<br><br>\n",
    "&nbsp;신경망 학습의 원래 목표는 범용적인 능력을 익히는 것이다. 범용 능력을 평가하려면 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해봐야 한다. 이를 위해 학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록해보자. 여기에서는 1에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록한다.<br><br>\n",
    "**NOTE_** 에폭은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'하게 된다. 이 경우 100회가 1에폭이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5cfbd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10441666666666667, 0.1028\n",
      "train acc, test acc | 0.7986833333333333, 0.8024\n",
      "train acc, test acc | 0.8765833333333334, 0.8811\n",
      "train acc, test acc | 0.89795, 0.9014\n",
      "train acc, test acc | 0.9071666666666667, 0.9095\n",
      "train acc, test acc | 0.9143666666666667, 0.9164\n",
      "train acc, test acc | 0.91875, 0.9198\n",
      "train acc, test acc | 0.9243666666666667, 0.9265\n",
      "train acc, test acc | 0.9275666666666667, 0.9285\n",
      "train acc, test acc | 0.9311666666666667, 0.932\n",
      "train acc, test acc | 0.93435, 0.9351\n",
      "train acc, test acc | 0.9369666666666666, 0.9375\n",
      "train acc, test acc | 0.94005, 0.9394\n",
      "train acc, test acc | 0.94225, 0.9406\n",
      "train acc, test acc | 0.9438166666666666, 0.9428\n",
      "train acc, test acc | 0.9460333333333333, 0.9446\n",
      "train acc, test acc | 0.9477333333333333, 0.9459\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca30880",
   "metadata": {},
   "source": [
    "&nbsp;위 그래프를 보면 훈련 데이터에 대한 정확도와 시험 데이터에 대한 정확도에 차이가 없음을 알 수 있다. 이는 제대로 학습이 되었으며 오버피팅이 일어나지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4ba5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
