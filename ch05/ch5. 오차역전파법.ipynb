{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c40eff",
   "metadata": {},
   "source": [
    "https://doomed-lab.tistory.com/41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f4278",
   "metadata": {},
   "source": [
    "# 5.1 계산 그래프\n",
    "## 5.1.1 계산 그래프로 풀다\n",
    "&nbsp;계산 그래프는 계산 과정을 노드와 화살표로 표현한다. 그래프는 복수의 노드와 에지로 표현된다. 다음 문제를 풀어보자<br>\n",
    "**문제:** 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀습니다. 사과 1개에 100원, 귤은 1개 50원입니다. 소비세가 10%일 때 지불 금액을 구하세요.\n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FECAfA%2Fbtq4rI9sNJw%2FK4VP8XFJkJbaONionuOWwk%2Fimg.png\"  width=\"600px\" height=\"600px\"></img><br>\n",
    "그래프에서 계산을 왼쪽에서 오른쪽으로 진행하면 **순전파(forward propagation)**, 반대로 오른쪽에서 왼쪽으로 진행하면 **역전파(backward propagation)**이다.\n",
    "## 5.1.2 국소적 계산\n",
    "&nbsp; 국소적이란 자신과 직접 관계된 작은 범위라는 뜻이다. 국소적 계산은 결국 전체에서 어떤일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것이다. <br>아래는 국소적 계산의 예이다.\n",
    " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbYYf9U%2Fbtq4rdaMMiq%2FSOzYnRR5Ph5vk3J3JSYk31%2Fimg.png\"  width=\"600px\" height=\"600px\"></img><br>\n",
    "각 노드에서의 계산은 국소적 계산이다. 사과와 그 외의 물품 값을 더하는 계산(4000 + 200 -> 4200)은 4000이라는 숫자가 어떻게 계산되었느냐와는 상관없이, 단지 두 숫자를 더하면 된다. 각 노드는 자신과 관련한 계산 외에는 신경쓸 것이 없다.<br><br>\n",
    "&nbsp;국소적 계산은 복잡한 계산을 단순한 계산으로 쪼개어 다른 단계와는 독립적으로 계산을 수행한다.\n",
    "\n",
    "# 5.2 연쇄법칙\n",
    "## 5.2.1 계산 그래프의 역전파\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fpgceg%2Fbtq4p4yS538%2FZMY0o0vYqltqG0eMBhnF9K%2Fimg.png\"  width=\"400px\" height=\"400px\"></img><br>\n",
    "&nbsp;역전파의 계산 절차는 신호 E에 노드의 국소적 미분을 곱한 후 다음 노드로 전달하는 것이다.\n",
    "## 5.2.2 연쇄법칙이란?\n",
    "&nbsp;**합성 함수**란 여러 함수로 구성된 함수이다. 예를 들어 z = (x + y)^2 라는 식은 <br>z = t^2 <br>t = x + y<br>로 두 개의 식으로 구성된다.<br> &nbsp;연쇄법칙은 합성 함수의 미분에 대한 성질이다.<br>**합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.**\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcX497u%2Fbtq4oQgMXUI%2FKCBpSZXHF3iTWkXEUMy240%2Fimg.jpg\"  width=\"300px\" height=\"300px\"></img><br>\n",
    "## 5.2.3 연쇄법칙과 계산 그래프\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fvt0V9%2Fbtq4wv2xPgL%2F9yiFA73c8yRPDf06jNbaSK%2Fimg.png\"  width=\"900px\" height=\"900px\"></img><br>\n",
    "# 5.3 역전파\n",
    "## 5.3.1 덧셈 노드의 역전파 \n",
    "&nbsp;z = x + y라는 식을 대상으로 역전파를 진행<br><br>\n",
    "dz/dx = 1<br>\n",
    "dz/dy = 1<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FALdF6%2Fbtq4o7bw6t0%2FmKFdTtk0uSkkl9bkt9aosk%2Fimg.png\"  width=\"700px\" height=\"700px\"></img><br>\n",
    "&nbsp;상류에서 전해진 미분 값을 dL/dz이라 했는데 이는 아래 그림과 같이 최종적으로 L이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문이다. z = x + y 계산은 그 큰 계산 그래프의 중간 어딘가에 존재하고 상류로부터 dL/dz값이 전해진 것이다. <br>\n",
    "dL/dz와 1(dz/dx or dz/dy)을 곱해서 하류로 넘겨준다.\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQHqxl%2Fbtq4wv9jSuZ%2FmKpR0h8BJCknEpRkDKT600%2Fimg.png\"  width=\"700px\" height=\"700px\"></img><br>\n",
    "## 5.3.2 곱셈 노드의 역전파\n",
    "&nbsp;z = xy라는 식을 대상으로 역전파 진행 <br><br>\n",
    "dz/dx = y<br>\n",
    "dz/dy = x<br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbPDIj3%2Fbtq4qET11St%2FaPTgX1y6Dw4Wpz8qiJcm1K%2Fimg.png\"  width=\"700px\" \n",
    "height=\"700px\"></img><br>     \n",
    "곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.\n",
    "dL/dz와 y or x (dz/dx or dz/dy)을 곱해서 하류로 넘겨준다.\n",
    "# 5.4 단순한 계층 구현하기\n",
    "&nbsp;곱셈 노드를 'MulLayer', 덧셈 노드를 'AddLayer'라는 이름으로 구현\n",
    "## 5.4.1 곱셈 계층\n",
    "&nbsp;모든 계층은 forward()와 backward()라는 공통의 메서드(인터페이스)를 갖도록 구현할 것이다. forward()는 순전파, backward()는 역전파를 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebfe9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y #x와 y를 바꾼다\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e0cc0",
   "metadata": {},
   "source": [
    "곱셈 노드에서의 역전파는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 '서로 바꿔' 곱한 후 하류로 보낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef479b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파 \n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcc108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ceaef",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955bbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921be2c",
   "metadata": {},
   "source": [
    "forward(): 입력받은 두 인수 x, y를 더해서 반환한다.<br>\n",
    "backward(): 상류에서 내려온 미분(dout)을 그대로 하류로 흘린다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefcb42-35b9-4ab7-a2bc-afe3de11bba6",
   "metadata": {},
   "source": [
    "덧셈 계층과 곱셈 계층을 사용해 사과 2개와 귤 3개를 사는 밑에 그림의 상황을 구현해보자<br>\n",
    "<img src=\"https://compmath.korea.ac.kr/appmath2022/_images/bp_layer_add.png\"  width=\"600px\" height=\"600px\"></img><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872b691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) #(1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) #(2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)\n",
    "price = mul_tax_layer.forward(all_price, tax) #(4)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) #(4)\n",
    "dapple_price, dorange_price =  add_apple_orange_layer.backward(dall_price) #(3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9198c20",
   "metadata": {},
   "source": [
    "# 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a3c5c",
   "metadata": {},
   "source": [
    "## 5.5.2 ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26341388",
   "metadata": {},
   "source": [
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F706edd9c-aa40-4a4e-bbf7-e503aafea599%2Fimage.png\"  width=\"500px\" height=\"250px\"></img><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2Ff094bb3f-c425-4390-b715-be429399b5a4%2Fimage.png\"  width=\"500px\" height=\"250px\"></img><br>\n",
    "순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류에 보낸다. <Br>\n",
    "순전파 때의 입력인 x가 0보다 이하면 역전파는 하류로 신호를 보내지 않는다. <Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c687b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0 # dout의 self.mask가 True인 인덱스를 0으로 만듦\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8192f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2e0c9",
   "metadata": {},
   "source": [
    "## 5.5.2 Sigmoid 계층\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F7a33d325-5edc-4f32-b80a-62e87d6c1006%2Fimage.png\"  width=\"500px\" height=\"250px\"></img><br>\n",
    "Sigmoid 계층의 계산 그래프(순전파)는 위와 같다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf37b84",
   "metadata": {},
   "source": [
    "### 1 단계\n",
    "'/'노드, 즉 y = 1/x 를 미분하면 다음과 같다.<br><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2Fca464272-2ab5-4d72-a982-5c6615ee2bad%2Fimage.png\"  width=\"250px\" height=\"250px\"></img><br><br>\n",
    "역전파 때는 상류에서 흘러온 값에 -y^2(순전파의 출력을 제곱한 후 마이너스를 붙인 값)을 곱해서 하류로 전달한다. <br><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2Fcccb0fc6-f119-4ef5-b4c5-bd1239bdc859%2Fimage.png\"  width=\"700px\" height=\"250px\"></img><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75b1a",
   "metadata": {},
   "source": [
    "### 2 단계\n",
    "'+' 노드는 상류의 값을 여과 없이 하류로 내보낸다.<br><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F54fd0786-e7e8-452a-ad92-cef02204a9cf%2Fimage.png\"  width=\"700px\" height=\"250px\"></img><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377766a3",
   "metadata": {},
   "source": [
    "### 3단계\n",
    "'exp' 노드는 y = exp(x) 연산을 수행하며, 그 미분은 다음과 같다.<br><br>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLokcldPWxZRFc43pdGOKaTy2TVPOKFduFgw&s\"  width=\"500px\" height=\"250px\"></img><br><br>\n",
    "계산 그래프에서는 상류의 값에 순전파 때의 출력(이 예에서는 exp(-x))을 곱해 하류로 전달한다.<br><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2Fbc40d0e2-f877-4da6-9d7e-a496e0ea4c76%2Fimage.png\"  width=\"700px\" height=\"250px\"></img><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e2930",
   "metadata": {},
   "source": [
    "### 4 단계\n",
    "'x' 노드는 순전파 때의 값을 '서로 바꿔' 곱한다.<br><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vj55VOGyOGtBXrNva5sYxQ.png\"  width=\"700px\" height=\"250px\"></img><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85726d5d",
   "metadata": {},
   "source": [
    "이상으로 Sigmoid 계층의 역전파를 계산 그래프로 완료했다. 이 과정을 단순하게 아래와 같이 나타낼 수 있다.<br><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*pBItjr8TXZzzZX94t-L9QQ.png\"  width=\"700px\" height=\"250px\"></img><br><br>\n",
    "더 나아가 Sigmoid 역전파의 출력값은 아래와 같이 정리된다.<br><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2c1tR8qiYNVU8ahAhctLCQ.png\"  width=\"700px\" height=\"250px\"></img><br><br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*8pK9G6CfrnYmhosUpuWxyA.png\"  width=\"700px\" height=\"250px\"></img><br><br> \n",
    "Sigmoid 계층을 코드로 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b8432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        out = self.out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c587105",
   "metadata": {},
   "source": [
    "# 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc00ba3",
   "metadata": {},
   "source": [
    "## 5.6.1 Affine 계층\n",
    "딥러닝에서 Affine 계층은 순전파시 행렬 곱을 수행하는 계층을 의미한다.<br><Br>\n",
    "신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 내적(넘파이에서는 np.dot())을 사용했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c393bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[1.46506686 1.81576824 0.72815473]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2) # 입력\n",
    "W = np.random.rand(2, 3) # 가중치\n",
    "B = np.random.rand(3) # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092860af",
   "metadata": {},
   "source": [
    "행렬의 내적은 차원의 원소 수를 일치시키는 게 핵심이다.<Br><BR>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqrU3W%2FbtqCrjDfKEM%2F9f1GtWxb22qMLYqe8URve0%2Fimg.png\"  width=\"700px\" height=\"300px\"></img><br><br> \n",
    "신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환(affine transformation)이라고 한다. 이 책에서는 어파인 변환을 수행하는 처리를 'Affine 계층'이라는 이름으로 구현한다.<br><br>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc1sdRF%2FbtqZ4AIXj0r%2FErfuZFksbr89toA3qRt2Zk%2Fimg.png\"  width=\"700px\" height=\"400px\"></img><br><br> \n",
    "어파인 계층의 순전파는 위와 같다. \n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb38TTs%2Fbtq0efJLhR1%2FijllTc3h61dL0h1t1LR3D0%2Fimg.png\"  width=\"700px\" height=\"300px\"></img><br><br> \n",
    "어파인 계층의 역전파는 위와 같다.<br>\n",
    "dot의 역전파는 곱셈의 역전파처럼 서로 바꿔 곱하는 것 같은데 이때 전치(T)를 하고 진행하는듯하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08115e",
   "metadata": {},
   "source": [
    "## 5.6.2 배치용 Affine 계층\n",
    "데이터 N개를 묶어 순전파하는 경우, 즉 배치용 Affine 계층을 생각해보자(묶은 데이터를 '배치'라고 부른다.)<br><br>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F78da8c8d-e61a-4f10-8c79-d0b952cbca18%2Fimage.png\"  width=\"700px\" height=\"500px\"></img><br><br> \n",
    "X의 형상이 (N, 2)가 된 것뿐이다. <br>\n",
    "편향을 더할 때도 주의해야한다. 순전파 때의 편향 덧셈은 X·Y에 대한 편향이 각 데이터에 더해진다. 예를 들어 N = 2(데이터가 2개)로 한 경우, 편향은 그 두 데이터 각각에(각각의 계산 결과에) 더해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84eeb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print()\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96b4a",
   "metadata": {},
   "source": [
    "위 결과와 같이 순전파의 편향 덧셈은 각각의 데이터(1번째 데이터, 2번째 데이터, ...)에 더진다. 그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdf8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "print()\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86209682",
   "metadata": {},
   "source": [
    "이 예에서는 데이터가 2개(N = 2)라고 가정한다. 편향의 역전파는 그 두 데이터에 대한 미분을 데이터마다 더해서 구한다. 그래서 np.sum()에서 0번째 축(데이터를 단위로 한축)에 대해서 (axis=0)의 총합을 구한다.<br><BR>\n",
    "Affine 구현은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea84832",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax-with-Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d54d1",
   "metadata": {},
   "source": [
    "소프트맥스 함수는 출력층에서 주로 사용한다. 소프트맥스 함수는 입력 값을 정규화하여 출력한다. 예를 들어 손글씨 숫자 인식에서의 Softmax 계층의 출력은 아래와 같다.<br><BR>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F746082a0-4403-458a-b275-ff7845e25c43%2Fimage.png\"  width=\"1000px\" height=\"500px\"></img><br><br> \n",
    "위와 같이 Softmax 계층은 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력한다. 또한, 손글씨 숫자는 가짓수가 10개(10클래스 분류)이므로 Softmax 계층의 입력은 10개가 된다.<Br><BR>\n",
    "신경망에서 수행하는 작업은 \"학습\"과 \"추론\" 두 가지이다. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. 그림에서 처럼 Affine 계층의 출력값(정규화 하지 않은 출력 결과)을 점수(score)라고 한다. 신경망 추론에서 답을 하나만 내는 경우 점수값이 가장 높은 것을 고르면 됨으로 굳이 Softmax 계층을 사용하지 않는다. 반면, 신경망을 학습할 때는 Softmax 계층이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd86ae",
   "metadata": {},
   "source": [
    "손실 함수인 교차 엔트로피 오차도 포함하여 'Sofrmax-with-Loss 계층'이라는 이름으로 구현한다. 아래는 Softmax-with-Loss 계층의 계산 그래프이다.<br><BR>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2Fd48f7376-1ff5-4423-8633-01217ae4bc6f%2Fimage.png\"></img><br><br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d4d12",
   "metadata": {},
   "source": [
    "위 그림에서와 같이 Softmax 계층은 입력 (a1, a2, a3)를 정규화하여 (y1, y2, y3)를 출력한다. Cross Entropy Error 계층은 Softmax 계층의 역전파는 (y1 - t1, y2 - t2, y3 - t3)이다. 그 이유는 (y1, y2, y3)는 Softmax 계층의 출력이고 (t1, t2, t3)는 정답 레이블이므로 (y1 - t1, y2 - t2, y3 - t3)는 Softmax 계층의 출력과 정답 레이블의 차분이기 때문이다. 신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해진다. <Br><BR>\n",
    "신경망 학습의 목적은 신경망의 출력(Softmax의 출력)이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이다. 그래서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야한다. (y1 - t1, y2 - t2, y3 - t3)라는 결과는 바로 Softmax 계층의 출력과 정답 레이블의 차이로, 신경망의 현재 출력과 정답 레이블의 오차를 있는 그대로 드러내는 것이다.<Br><BR>\n",
    "예를 들어보자<br>\n",
    "정답 레이블이 (0, 1, 0)일 때 Softmax의 출력값이 (0.3, 0.2, 0.5)라고 하자. 이때 Softmax 계층의 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파한다. 학습하는 정도가 커진다.<br>\n",
    "마찬가지로 정답 레이블이 (0, 1, 0)일 때 Softmax의 출력값이 (0.01, -0.01, 0)라고 하자. 이때 Softmax 계층의 역전파는 (0.01, -0.01, 0)라는 비교적 작은 오차를 전달한다. 학습하는 정도도 작아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9974fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)    # 오버플로 대책\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8737b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_k, t_k):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t_k * np.log(y_k + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e7aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72747c2",
   "metadata": {},
   "source": [
    "역전파 때는 전파하는 값을 배치의 수(batch_size)로 나눠서 데이터 1개당 오차를 앞 계층으로 전파한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2500ed",
   "metadata": {},
   "source": [
    "# 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c97af2",
   "metadata": {},
   "source": [
    "## 5.7.1 신경망 학습의 전체 그림\n",
    "다음은 신경망 학습의 순서이다.<Br><BR>\n",
    "- 전제 <br>\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행된다.<br><br>\n",
    "1. 미니배치<bR>\n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다. <br><br>\n",
    "2. 기울기 산출<br>\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.<br><br>\n",
    "3. 매개변수 갱신<br>\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.<br><BR>\n",
    "4. 반복<BR>\n",
    "1~3단계를 반복한다.<BR><BR>\n",
    "지금까지 설명한 오차역전파법이 등장하는 단계는 두 번째인 '기울기 산출'이다. 기울기를 구하기 위해 수치 미분을 사용했는데, 이는 구현은 쉽지만 계산이 오래걸린다. 오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적으로 빠르게 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a74d5",
   "metadata": {},
   "source": [
    "## 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "2층 신경망을 TwoLayerNet 클래스로 구현한다. 이 클래스의 인스턴스 변수와 메서드를 정리하면 다음과 같다.<Br><BR>\n",
    "<img src=\"https://velog.velcdn.com/images%2Fkimkihoon0515%2Fpost%2F308e673e-54b0-4462-b4f9-e2db6d592433%2Fimage.png\"></img><br><br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfec0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a403e3",
   "metadata": {},
   "source": [
    "신경망의 계층을 OrderedDict에 보관하는 점이 중요하다. OrderedDict은 순서가 있는 딕셔너리이다. '순서가 있는'이란 딕셔너리에 추가한 순서를 기억한다는 것이다. 그래서 순전파 때는 추가한 순서대로 각 계층의 forward() 메서드를 호출하기만 하면 처리가 완료된다. 마찬가지로 역전파 때는 계층을 밴다 순서로 호출하기만 하면 된다. Affine 계층과 ReLU 계층이 각자의 내부에서 순전파와 역전파를 제대로 처리하고 있으니, 여기에서는 그냥 계층을 올바른 순서로 연결한 다음 순서대로(혹은 역순으로) 호출해주면 끝이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8efbf7",
   "metadata": {},
   "source": [
    "## 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "기울기를 구하는 방법은 수치 미분을 써서 구하는 방법, 해석적으로 수식을 풀어 구하는 방법 총 두 가지이다. <Br>\n",
    "수치 미분은 구현이 쉬워서 버그가 발생할 확률이 적다. 오차역전파법은 구현하기 복잡해서 종종 버그가 발견되곤 한다. 수치 미분과 오차역전파법의 결과값이 일치한지를 비교하여 기울기 검증을 할 수 있다. 이를 기울기 확인(gradient check)이라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f002cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.645385532290286e-10\n",
      "b1:2.6057988853809503e-09\n",
      "W2:5.677007515649507e-09\n",
      "b2:1.4052772058376072e-07\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2523e",
   "metadata": {},
   "source": [
    "MNIST 데이터셋을 읽는다. 그리고 훈련 데이터 일부를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차를 확인한다. 각 가중치 매개변수의 차이의 절댓값을 구하고, 이를 평균한 값이 오차가 된다.<BR><BR>\n",
    "위 코드의 실행결과를 해석하면 다음과 같다.<BR><BR>\n",
    "수치 미분과 오차역전파법으로 구한 기울기의 차이는 매우 작다. <BR>예를 들어 <BR>W1:4.6e-10<BR>의 의미는 4.6e-10(0.00000000046)으로 매우 작다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb8577",
   "metadata": {},
   "source": [
    "## 5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "오차역전파법을 사용한 신경망을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b88790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08535 0.0907\n",
      "0.9048333333333334 0.9086\n",
      "0.9243166666666667 0.9262\n",
      "0.9344666666666667 0.9344\n",
      "0.9428833333333333 0.941\n",
      "0.9499666666666666 0.9479\n",
      "0.9548166666666666 0.9499\n",
      "0.9591833333333334 0.9545\n",
      "0.9587166666666667 0.957\n",
      "0.9659333333333333 0.9608\n",
      "0.9672166666666666 0.9619\n",
      "0.9707166666666667 0.9652\n",
      "0.97245 0.9664\n",
      "0.9743666666666667 0.9664\n",
      "0.9742333333333333 0.9661\n",
      "0.9766166666666667 0.9677\n",
      "0.9776666666666667 0.9675\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
